{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJWi02+UA9Nd2K6XE7L8qF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thenaivekid/Implementation-of-VideoMAE-for-Video-Classification/blob/main/Implementatioin_of_Video_MAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WpcyazC85Za"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import Optional, Tuple, Union, Callable, List, Dict, Any\n",
        "import collections.abc\n",
        "import json # For loading config from string/file\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class BaseModelOutput:\n",
        "    last_hidden_state: Any\n",
        "    hidden_states: Optional[Tuple[Any, ...]] = None\n",
        "    attentions: Optional[Tuple[Any, ...]] = None\n",
        "# --- Configuration Class ---\n",
        "class VideoMAEConfig:\n",
        "    def __init__(self, **kwargs):\n",
        "        # Essential parameters (with defaults for standalone use if not all are in kwargs)\n",
        "        self.image_size: Union[int, Tuple[int, int]] = kwargs.get(\"image_size\", 224)\n",
        "        self.patch_size: Union[int, Tuple[int, int]] = kwargs.get(\"patch_size\", 16)\n",
        "        self.num_channels: int = kwargs.get(\"num_channels\", 3)\n",
        "        self.hidden_size: int = kwargs.get(\"hidden_size\", 768)\n",
        "        self.num_frames: int = kwargs.get(\"num_frames\", 16)\n",
        "        self.tubelet_size: int = kwargs.get(\"tubelet_size\", 2)\n",
        "\n",
        "        self.num_hidden_layers: int = kwargs.get(\"num_hidden_layers\", 12)\n",
        "        self.num_attention_heads: int = kwargs.get(\"num_attention_heads\", 12)\n",
        "        self.intermediate_size: int = kwargs.get(\"intermediate_size\", 3072)\n",
        "\n",
        "        self.hidden_act: str = kwargs.get(\"hidden_act\", \"gelu\")\n",
        "        self.hidden_dropout_prob: float = kwargs.get(\"hidden_dropout_prob\", 0.0)\n",
        "        self.attention_probs_dropout_prob: float = kwargs.get(\"attention_probs_dropout_prob\", 0.0)\n",
        "        self.qkv_bias: bool = kwargs.get(\"qkv_bias\", True)\n",
        "        self.layer_norm_eps: float = kwargs.get(\"layer_norm_eps\", 1e-12)\n",
        "\n",
        "        self.initializer_range: float = kwargs.get(\"initializer_range\", 0.02)\n",
        "        self.use_mean_pooling: bool = kwargs.get(\"use_mean_pooling\", True)\n",
        "\n",
        "        self.id2label: Optional[Dict[int, str]] = kwargs.get(\"id2label\", None)\n",
        "        self.label2id: Optional[Dict[str, int]] = kwargs.get(\"label2id\", None)\n",
        "        self.num_labels: Optional[int] = kwargs.get(\"num_labels\", None)\n",
        "        if self.num_labels is None and self.id2label is not None:\n",
        "            self.num_labels = len(self.id2label)\n",
        "\n",
        "        self.problem_type: Optional[str] = kwargs.get(\"problem_type\", None)\n",
        "\n",
        "        # Allow any other kwargs to be set as attributes\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict: Dict[str, Any]):\n",
        "        return cls(**config_dict)\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file_path: str):\n",
        "        with open(json_file_path, 'r') as f:\n",
        "            config_dict = json.load(f)\n",
        "        return cls(**config_dict)\n",
        "\n",
        "# --- Helper: Sinusoid Encoding ---\n",
        "def get_sinusoid_encoding_table(n_position, d_hid):\n",
        "    \"\"\"Sinusoid position encoding table\"\"\"\n",
        "    def get_position_angle_vec(position):\n",
        "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
        "\n",
        "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
        "\n",
        "# --- Modules ---\n",
        "class VideoMAEPatchEmbeddings(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        image_size = config.image_size\n",
        "        patch_size = config.patch_size\n",
        "\n",
        "        image_size_tuple = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
        "        patch_size_tuple = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
        "\n",
        "        self.image_size = image_size_tuple\n",
        "        self.patch_size = patch_size_tuple\n",
        "        self.tubelet_size = int(config.tubelet_size)\n",
        "        self.num_frames = config.num_frames\n",
        "        self.num_channels = config.num_channels\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        num_patches_per_frame = (self.image_size[0] // self.patch_size[0]) * \\\n",
        "                                (self.image_size[1] // self.patch_size[1])\n",
        "        num_temporal_patches = self.num_frames // self.tubelet_size\n",
        "        self.num_patches = num_patches_per_frame * num_temporal_patches\n",
        "\n",
        "        self.projection = nn.Conv3d(\n",
        "            in_channels=self.num_channels,\n",
        "            out_channels=self.hidden_size,\n",
        "            kernel_size=(self.tubelet_size, self.patch_size[0], self.patch_size[1]),\n",
        "            stride=(self.tubelet_size, self.patch_size[0], self.patch_size[1]),\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
        "        if num_channels != self.num_channels:\n",
        "            raise ValueError(\"Channel mismatch\")\n",
        "        if height != self.image_size[0] or width != self.image_size[1]:\n",
        "            raise ValueError(f\"Input image size mismatch\")\n",
        "        if num_frames != self.num_frames:\n",
        "            raise ValueError(f\"Input frame count mismatch\")\n",
        "\n",
        "        pixel_values = pixel_values.permute(0, 2, 1, 3, 4)\n",
        "        embeddings = self.projection(pixel_values)\n",
        "        embeddings = embeddings.flatten(2).transpose(1, 2)\n",
        "        return embeddings\n",
        "\n",
        "class VideoMAEEmbeddings(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.patch_embeddings = VideoMAEPatchEmbeddings(config)\n",
        "        self.num_patches = self.patch_embeddings.num_patches\n",
        "        self.position_embeddings_table = get_sinusoid_encoding_table(self.num_patches, config.hidden_size)\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        embeddings = self.patch_embeddings(pixel_values)\n",
        "        pos_embed = self.position_embeddings_table.type_as(embeddings).to(device=embeddings.device, copy=True)\n",
        "        embeddings = embeddings + pos_embed\n",
        "        if bool_masked_pos is not None:\n",
        "            batch_size, _, num_channels_emb = embeddings.shape\n",
        "            # Simplified handling for bool_masked_pos as in original.\n",
        "            # This part is typically for MAE pre-training.\n",
        "            embeddings = embeddings[~bool_masked_pos].reshape(batch_size, -1, num_channels_emb)\n",
        "        return embeddings\n",
        "\n",
        "def eager_attention_forward(\n",
        "    query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
        "    attention_mask: Optional[torch.Tensor], scaling: float,\n",
        "    dropout_p: float = 0.0, training: bool = False,\n",
        "):\n",
        "    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
        "    attn_weights = nn.functional.dropout(attn_weights, p=dropout_p, training=training)\n",
        "    if attention_mask is not None:\n",
        "        attn_weights = attn_weights * attention_mask\n",
        "    attn_output = torch.matmul(attn_weights, value)\n",
        "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "    return attn_output, attn_weights\n",
        "\n",
        "class VideoMAESelfAttention(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\"Hidden size not multiple of num_attention_heads\")\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.dropout_prob = config.attention_probs_dropout_prob\n",
        "        self.scaling = self.attention_head_size**-0.5\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n",
        "\n",
        "        self.q_bias = None\n",
        "        self.v_bias = None\n",
        "        if config.qkv_bias:\n",
        "            self.q_bias = nn.Parameter(torch.zeros(self.all_head_size))\n",
        "            self.v_bias = nn.Parameter(torch.zeros(self.all_head_size))\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self, hidden_states: torch.Tensor,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False\n",
        "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
        "        k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None and self.v_bias is not None else None\n",
        "\n",
        "        queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)\n",
        "        keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)\n",
        "        values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(queries)\n",
        "        key_layer = self.transpose_for_scores(keys)\n",
        "        value_layer = self.transpose_for_scores(values)\n",
        "\n",
        "        context_layer, attention_probs = eager_attention_forward(\n",
        "            query=query_layer, key=key_layer, value=value_layer,\n",
        "            attention_mask=head_mask, scaling=self.scaling,\n",
        "            dropout_p=self.dropout_prob, training=self.training,\n",
        "        )\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.reshape(new_context_layer_shape)\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "        return outputs\n",
        "\n",
        "class VideoMAESelfOutput(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        # input_tensor is not strictly needed here for VideoMAE logic, but kept for potential compatibility\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "class VideoMAEAttention(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.attention = VideoMAESelfAttention(config)\n",
        "        self.output = VideoMAESelfOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self, hidden_states: torch.Tensor,\n",
        "        head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False,\n",
        "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
        "\n",
        "        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
        "        attention_output = self.output(self_outputs[0], hidden_states) # Pass hidden_states for API consistency\n",
        "        outputs = (attention_output,) + self_outputs[1:]\n",
        "        return outputs\n",
        "\n",
        "class VideoMAEIntermediate(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if config.hidden_act == \"gelu\":\n",
        "            self.intermediate_act_fn = nn.GELU()\n",
        "        elif config.hidden_act == \"relu\":\n",
        "            self.intermediate_act_fn = nn.ReLU()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {config.hidden_act}\")\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "class VideoMAEOutput(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = hidden_states + input_tensor\n",
        "        return hidden_states\n",
        "\n",
        "class VideoMAELayer(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.attention = VideoMAEAttention(config)\n",
        "        self.intermediate = VideoMAEIntermediate(config) # this is up scaling part of mlp\n",
        "        self.output = VideoMAEOutput(config) # this is second part of mlp\n",
        "        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self, hidden_states: torch.Tensor,\n",
        "        head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False,\n",
        "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
        "\n",
        "        normed_hidden_states = self.layernorm_before(hidden_states)\n",
        "        self_attention_outputs = self.attention(\n",
        "            normed_hidden_states, head_mask, output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        outputs = self_attention_outputs[1:]\n",
        "        hidden_states = attention_output + hidden_states # First residual\n",
        "\n",
        "        normed_hidden_states_after_attn = self.layernorm_after(hidden_states)\n",
        "        intermediate_output = self.intermediate(normed_hidden_states_after_attn)\n",
        "        layer_output = self.output(intermediate_output, hidden_states) # Second residual inside self.output\n",
        "        outputs = (layer_output,) + outputs\n",
        "        return outputs\n",
        "\n",
        "class VideoMAEEncoder(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([VideoMAELayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self, hidden_states: torch.Tensor,\n",
        "        head_mask: Optional[List[Optional[torch.Tensor]]] = None,\n",
        "        output_attentions: bool = False, output_hidden_states: bool = False,\n",
        "        return_dict: bool = True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None and i < len(head_mask) else None\n",
        "            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class ImageClassifierOutput():\n",
        "    def __init__(self, loss=None, logits=None, hidden_states=None, attentions=None):\n",
        "        self.loss = loss\n",
        "        self.logits = logits\n",
        "        self.hidden_states = hidden_states\n",
        "        self.attentions = attentions\n",
        "    def __repr__(self):\n",
        "        parts = []\n",
        "        if self.loss is not None: parts.append(f\"loss={self.loss.item() if isinstance(self.loss, torch.Tensor) else self.loss}\")\n",
        "        if self.logits is not None: parts.append(f\"logits.shape={self.logits.shape}\")\n",
        "        if self.hidden_states is not None: parts.append(f\"hidden_states.len={len(self.hidden_states)}\")\n",
        "        if self.attentions is not None: parts.append(f\"attentions.len={len(self.attentions)}\")\n",
        "        return f\"ImageClassifierOutput({', '.join(parts)})\"\n",
        "\n",
        "\n",
        "class VideoMAEModel(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = VideoMAEEmbeddings(config)\n",
        "        self.encoder = VideoMAEEncoder(config)\n",
        "\n",
        "        # self.fc_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) if config.use_mean_pooling else None\n",
        "        if config.use_mean_pooling:\n",
        "            self.layernorm = None\n",
        "        else:\n",
        "            self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "        # # Initialize weights and apply final processing\n",
        "        # self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.patch_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.FloatTensor,\n",
        "        bool_masked_pos: Optional[torch.BoolTensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ):\n",
        "\n",
        "\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        # head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(pixel_values, bool_masked_pos)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "        sequence_output = encoder_outputs.last_hidden_state\n",
        "        if self.layernorm is not None:\n",
        "            sequence_output = self.layernorm(sequence_output)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output,) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=sequence_output,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "class VideoMAEForVideoClassification(nn.Module):\n",
        "    def __init__(self, config: VideoMAEConfig):\n",
        "        super().__init__()\n",
        "        self.config = config # Store config\n",
        "\n",
        "        if config.num_labels is None:\n",
        "            raise ValueError(\"config.num_labels must be set, e.g., from len(config.id2label).\")\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        # Use VideoMAEModel as the base, rename it to self.videomae\n",
        "        self.videomae = VideoMAEModel(config)\n",
        "\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.num_labels) if self.num_labels > 0 else nn.Identity()\n",
        "        if config.use_mean_pooling:\n",
        "            self.fc_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        else:\n",
        "            self.fc_norm = None\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self, module: Optional[nn.Module] = None):\n",
        "        if module is None: module = self\n",
        "        if isinstance(module, (nn.Linear, nn.Conv3d)):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None: module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        for child in module.children():\n",
        "            if child is not module: self._init_weights(child)\n",
        "\n",
        "    def forward(\n",
        "        self, pixel_values: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[List[Optional[torch.Tensor]]] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "    ):\n",
        "        # Use the renamed self.videomae\n",
        "        outputs = self.videomae(\n",
        "            pixel_values,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=True, # Ensure we get a dict-like output\n",
        "        )\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "\n",
        "\n",
        "        if self.fc_norm is not None: # Corresponds to use_mean_pooling = True\n",
        "            pooled_output = sequence_output.mean(dim=1)\n",
        "            sequence_output = self.fc_norm(pooled_output)\n",
        "        else:\n",
        "            sequence_output = sequence_output[:, 0] # Assumes CLS token or similar logic\n",
        "\n",
        "        logits = self.classifier(sequence_output)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            current_problem_type = self.config.problem_type\n",
        "            if current_problem_type is None:\n",
        "                if self.num_labels == 1: current_problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    current_problem_type = \"single_label_classification\"\n",
        "                else: current_problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if current_problem_type == \"regression\":\n",
        "                loss_fct = nn.MSELoss()\n",
        "                loss = loss_fct(logits.squeeze(), labels.squeeze().float()) if self.num_labels == 1 else loss_fct(logits, labels.float())\n",
        "            elif current_problem_type == \"single_label_classification\":\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif current_problem_type == \"multi_label_classification\":\n",
        "                loss_fct = nn.BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels.float())\n",
        "\n",
        "        hidden_s = outputs.hidden_states\n",
        "        attns = outputs.attentions\n",
        "\n",
        "\n",
        "        return ImageClassifierOutput(\n",
        "            loss=loss, logits=logits, hidden_states=hidden_s, attentions=attns,\n",
        "        )\n",
        "\n",
        "from safetensors.torch import load_file\n",
        "config_json_str = \"\"\"\n",
        "{\n",
        "    \"architectures\": [\"VideoMAEForVideoClassification\"],\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"decoder_hidden_size\": 384,\n",
        "    \"decoder_intermediate_size\": 1536,\n",
        "    \"decoder_num_attention_heads\": 6,\n",
        "    \"decoder_num_hidden_layers\": 4,\n",
        "    \"hidden_act\": \"gelu\",\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"hidden_size\": 768,\n",
        "    \"id2label\": {\n",
        "        \"0\": \"drink water\", \"1\": \"brush teeth\", \"2\": \"pick up\", \"3\": \"reading\", \"4\": \"writing\",\n",
        "        \"5\": \"cheer up\", \"6\": \"jump up\", \"7\": \"phone call\", \"8\": \"taking a selfie\", \"9\": \"salute\"\n",
        "    },\n",
        "    \"image_size\": 224,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 3072,\n",
        "    \"label2id\": {\n",
        "        \"drink water\": 0, \"brush teeth\": 1, \"pick up\": 2, \"reading\": 3, \"writing\": 4,\n",
        "        \"cheer up\": 5, \"jump up\": 6, \"phone call\": 7, \"taking a selfie\": 8, \"salute\": 9\n",
        "    },\n",
        "    \"layer_norm_eps\": 1e-12,\n",
        "    \"model_type\": \"videomae\",\n",
        "    \"norm_pix_loss\": false,\n",
        "    \"num_attention_heads\": 12,\n",
        "    \"num_channels\": 3,\n",
        "    \"num_frames\": 16,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"patch_size\": 16,\n",
        "    \"qkv_bias\": true,\n",
        "    \"torch_dtype\": \"float32\",\n",
        "    \"transformers_version\": \"4.21.0.dev0\",\n",
        "    \"tubelet_size\": 2,\n",
        "    \"use_mean_pooling\": true\n",
        "}\n",
        "\"\"\"\n",
        "config_dict = json.loads(config_json_str)\n",
        "\n",
        "# For testing, reduce layers for speed\n",
        "# config_dict[\"num_hidden_layers\"] = 2\n",
        "# config_dict[\"hidden_size\"] = 128\n",
        "# config_dict[\"intermediate_size\"] = 256\n",
        "# config_dict[\"num_attention_heads\"] = 4\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "mae_config = VideoMAEConfig.from_dict(config_dict)\n",
        "\n",
        "batch_size = 2\n",
        "# Model expects (B, NumFrames, NumChannels, H, W)\n",
        "video_input = torch.randn(batch_size, mae_config.num_frames, mae_config.num_channels,\n",
        "                          mae_config.image_size, mae_config.image_size)\n",
        "# dummy_labels = torch.randint(0, mae_config.num_labels, (batch_size,))\n",
        "\n",
        "print(f\"Input video shape: {video_input.shape}\")\n",
        "print(f\"Number of labels from config: {mae_config.num_labels}\")\n",
        "\n",
        "model = VideoMAEForVideoClassification(mae_config) # Instantiate the model\n",
        "\n",
        "model = model.to(device)\n",
        "video_input = video_input.to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(video_input)\n",
        "\n",
        "print(f\"\\nOutput type: {type(outputs)}\")\n",
        "print(outputs)\n",
        "\n",
        "if outputs.loss is not None:\n",
        "    print(f\"Calculated loss: {outputs.loss.item()}\")\n",
        "\n",
        "if outputs.hidden_states:\n",
        "    print(f\"Number of hidden_states layers: {len(outputs.hidden_states)}\")\n",
        "    print(f\"Shape of first hidden_state: {outputs.hidden_states[0].shape}\")\n",
        "    print(f\"Shape of last hidden_state (from encoder): {outputs.hidden_states[-1].shape}\")\n",
        "if outputs.attentions:\n",
        "    print(f\"Number of attention layers: {len(outputs.attentions)}\")\n",
        "    # Note: Attention output shape from eager_attention_forward is (B, H, N, N)\n",
        "    # After VideoMAESelfAttention, it's still this shape when output_attentions=True\n",
        "    print(f\"Shape of first attention weights: {outputs.attentions[0].shape}\")\n",
        "\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    }
  ]
}